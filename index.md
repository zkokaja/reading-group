# NLP Journal Club

List of papers and resources for the papers we're reading.

## NLP Spring 2021

| Date | Paper |
| ---- | ----- |
| 4/15 | [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)

TBD:
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
- [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf)
- [Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents](https://arxiv.org/pdf/2103.13552.pdf)
- [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)
- [Language Models are Open Knowledge Graphs](https://arxiv.org/abs/2010.11967)

## NLP Summer 2020

| Date | Paper |
| ---- | ----- |
| 9/3  | [Climbing towards NLU](https://www.aclweb.org/anthology/2020.acl-main.463.pdf)
| 8/20 | [Sentence Simplification with Deep Reinforcement Learning](https://www.aclweb.org/anthology/D17-1062.pdf)
| 8/13 | [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf)
| 7/30 | [CausaLM: Causal Model Explanation Through Counterfactual Language Models](https://arxiv.org/pdf/2005.13407.pdf)
| 7/23 | [The Mythos of Model Interpretability](https://dl.acm.org/doi/pdf/10.1145/3236386.3241340)
| 7/9  | [Attention is not not Explanation](https://arxiv.org/pdf/1908.04626.pdf)
| 6/25 | [Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)](https://arxiv.org/pdf/1905.11833.pdf)
| 6/18 | [Generating Long Sequences with Sparse Transformers](https://arxiv.org/pdf/1904.10509.pdf)
| 6/11 | [UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/pdf/1909.11740.pdf)
| 6/4  | [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)

## NLP Spring 2020

| Date | Paper |
| ---- | ----- |
| 5/28 | [Robust and Scalable Differentiable Neural Computer for Question Answering](https://www.aclweb.org/anthology/W18-2606.pdf)
| 5/21 | [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)
| 5/14 | [WT5?! Training Text-to-Text Models to Explain their Predictions](https://arxiv.org/pdf/2004.14546.pdf)
| 5/7  | [Deep Communicating Agents for Abstractive Summarization](https://www.aclweb.org/anthology/N18-1150.pdf)
| 4/30 | [Pay Less Attention with Lightweight and Dynamic Convolutions](https://openreview.net/pdf?id=SkVhlh09tX)
| 4/23 | [The Evolved Transformer](https://arxiv.org/pdf/1901.11117.pdf)
| 4/16 | [ERNIE](https://arxiv.org/pdf/1905.07129.pdf) and [ERNIE 2.0](https://arxiv.org/pdf/1907.12412.pdf)
| 4/9  | [Meena](https://arxiv.org/abs/2001.09977)
| 4/2  | [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
| 3/26 | [BART](https://arxiv.org/pdf/1910.13461.pdf)
| 3/19 | [Multi-task Learning with Multi-head Attention for Multi-choice Reading Comprehension](https://arxiv.org/pdf/2003.04992.pdf)
| 3/12 | [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/pdf/2002.12327.pdf)
| 3/5  | [Compressive Transfomers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
