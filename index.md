# NLP Journal Club

List of papers and resources for the papers we're reading.

## NLP Spring 2021

| Date | Paper |
| ---- | ----- |
| 4/15 | [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)
| 4/29 | [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805)
| 5/13 | [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)
| 5/27 | [Predicting In-Game Actions from Interviews of NBA Players](https://direct.mit.edu/coli/article/46/3/667/93377/Predicting-In-Game-Actions-from-Interviews-of-NBA)
| 6/24 | [Decision Transformer: ReinforcementLearning via Sequence Modeling](https://arxiv.org/pdf/2106.01345.pdf)
| | [An autonomous debating system](https://www.nature.com/articles/s41586-021-03215-w)
| | [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf)
| | [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
| | [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf)
| | [Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents](https://arxiv.org/pdf/2103.13552.pdf)
| | [Language Models are Open Knowledge Graphs](https://arxiv.org/abs/2010.11967)
| | [Meta-learning for Few-shot Natural Language Processing: A Survey](https://arxiv.org/pdf/2007.09604.pdf)
| | [What Do Compressed Deep Neural Networks Forget?](https://arxiv.org/pdf/1911.05248.pdf)
| | [CAN A FRUIT FLY LEARN WORD EMBEDDINGS?](https://openreview.net/pdf?id=xfmSoxdxFCG)
| | [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/pdf/2105.03824.pdf)
| | [Unsupervised Speech Recognition](https://scontent.fsdv3-1.fna.fbcdn.net/v/t39.8562-6/187874612_311717527241594_5668815448923437055_n.pdf?_nc_cat=102&ccb=1-3&_nc_sid=ae5e01&_nc_ohc=TJVsTYPv8XQAX_KDPcT&_nc_ht=scontent.fsdv3-1.fna&oh=7489839a25bee73b388436c19d2df0d1&oe=60CDE095)
| | [Not All Memories are Created Equal: Learning to Forget by Expiring](https://arxiv.org/abs/2105.06548)
| | [Compositional Processing Emerges in Neural Networks Solving Math Problems](https://arxiv.org/pdf/2105.08961.pdf)
| | [John praised Mary because he? Implicit Causality Bias and Its Interaction with Explicit Cues in LM](https://arxiv.org/pdf/2106.01060.pdf)

## NLP Summer 2020

| Date | Paper |
| ---- | ----- |
| 9/3  | [Climbing towards NLU](https://www.aclweb.org/anthology/2020.acl-main.463.pdf)
| 8/20 | [Sentence Simplification with Deep Reinforcement Learning](https://www.aclweb.org/anthology/D17-1062.pdf)
| 8/13 | [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf)
| 7/30 | [CausaLM: Causal Model Explanation Through Counterfactual Language Models](https://arxiv.org/pdf/2005.13407.pdf)
| 7/23 | [The Mythos of Model Interpretability](https://dl.acm.org/doi/pdf/10.1145/3236386.3241340)
| 7/9  | [Attention is not not Explanation](https://arxiv.org/pdf/1908.04626.pdf)
| 6/25 | [Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)](https://arxiv.org/pdf/1905.11833.pdf)
| 6/18 | [Generating Long Sequences with Sparse Transformers](https://arxiv.org/pdf/1904.10509.pdf)
| 6/11 | [UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/pdf/1909.11740.pdf)
| 6/4  | [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)


## NLP Spring 2020

| Date | Paper |
| ---- | ----- |
| 5/28 | [Robust and Scalable Differentiable Neural Computer for Question Answering](https://www.aclweb.org/anthology/W18-2606.pdf)
| 5/21 | [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)
| 5/14 | [WT5?! Training Text-to-Text Models to Explain their Predictions](https://arxiv.org/pdf/2004.14546.pdf)
| 5/7  | [Deep Communicating Agents for Abstractive Summarization](https://www.aclweb.org/anthology/N18-1150.pdf)
| 4/30 | [Pay Less Attention with Lightweight and Dynamic Convolutions](https://openreview.net/pdf?id=SkVhlh09tX)
| 4/23 | [The Evolved Transformer](https://arxiv.org/pdf/1901.11117.pdf)
| 4/16 | [ERNIE](https://arxiv.org/pdf/1905.07129.pdf) and [ERNIE 2.0](https://arxiv.org/pdf/1907.12412.pdf)
| 4/9  | [Meena](https://arxiv.org/abs/2001.09977)
| 4/2  | [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)
| 3/26 | [BART](https://arxiv.org/pdf/1910.13461.pdf)
| 3/19 | [Multi-task Learning with Multi-head Attention for Multi-choice Reading Comprehension](https://arxiv.org/pdf/2003.04992.pdf)
| 3/12 | [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/pdf/2002.12327.pdf)
| 3/5  | [Compressive Transfomers for Long-Range Sequence Modelling](https://arxiv.org/abs/1911.05507)
